{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 01\n",
    "**Kernel Methods in Machine Learning (CS-E4830)**\n",
    "\n",
    "**Release date**: 16th of January, 2019\n",
    "\n",
    "**Submission date**: 31st of January, 2019 @4PM (no late submission allowed)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Tasks:</b>\n",
    "   \n",
    "1. [Implement kernel matrix calculation](#task_1) \n",
    " 1. [Linear kernel](#task_1_a) (**1 Point**)\n",
    " 2. [Gaussian kernel](#task_1_b) (**1 Point**)\n",
    "2. [Implement the Parzen Window Classifier](#task_2) (**2 Point**)\n",
    "3. [Application of the Parzen Window Classifier](#task_3)\n",
    " 1. [Implement the hyper parameter optimization](#task_3_a) (**1 Point**)\n",
    " 2. [Plot validation score for different hyper parameters](#task_3_b)\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Bonus Tasks:</b>\n",
    "    \n",
    "4. [Model visualization: Non-linear vs. Linear](#task_4) (**no points**)\n",
    "</div>\n",
    "\n",
    "**Version**: 1.2\n",
    "\n",
    "**Version history**:\n",
    "- 1.0: Initial version\n",
    "- 1.1: Fix typo: Parzen Window Clf. dual-variables (alpha) had wrong sign in formula. [Issue on MyCourses](https://mycourses.aalto.fi/mod/forum/discuss.php?d=128291)\n",
    "- 1.2: Fix typo: Gaussian kernel doc-string, put brackets around the denominator within exponential function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    Please add you student number and email address to the notebook into the corresponding cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMAIL: firstname.lastname@aalto.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDENT_NUMBER: 000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required python packages\n",
    "All tasks in this exercise can be solved by using only function and packages imported below. Please **do not** use any other imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d8b7dd6e83cc5aea17a517342e1a37",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "plt.set_cmap(\"RdBu_r\")\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.model_selection import KFold, ParameterGrid, BaseCrossValidator, train_test_split\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernel matrix calculation <a id='task_1'></a>\n",
    "Your task is to **implement two functions to calculate Linear- and Gaussian-kernel matrices** for two sets of feature vectors $\\mathbf{X}_A\\in\\mathbb{R}^{n_A\\times d}, \\mathbf{X}_B\\in\\mathbb{R}^{n_B\\times d}$, where $d$ is the dimension of the feature vectors, and $n_a$ and $n_B$ are the number of examples in set $A$ respectively $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Linear kernel (1 Point) <a id='task_1_a'></a>\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b>\n",
    "    Implement missing code parts of the function calculation the linear kernel matrix given two feature vector matrices $\\mathbf{X}_A$ and $\\mathbf{X}_B$. The resulting kernel matrix $\\mathbf{K}_{lin}$ must have dimension $n_A\\times n_B$. For a single entry in the kernel matrix it must hold:\n",
    "    <br/><br/>\n",
    "    $$[\\mathbf{K}_{lin}]_{ij}=\\kappa_{lin}(\\mathbf{x}_i,\\mathbf{x}_j)=\\langle\\mathbf{x}_i,\\mathbf{x}_j\\rangle,$$\n",
    "    <br/>\n",
    "    with $\\mathbf{x}_i,\\mathbf{x}_j\\in\\mathbb{R}^{d}$ being two examples from set $A$ respecively $B$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "399d1eacbf80766f7202f82001c34863",
     "grade": false,
     "grade_id": "linear_kernel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_kernel(X_A, X_B=None):\n",
    "    \"\"\"\n",
    "    Calculate linear kernel matrix between two sets of feature-vectors, so that:\n",
    "\n",
    "        k_ij = <x_i, x_j>\n",
    "\n",
    "    :param X_A: array-like, shape=(n_samples_A, d), feature-matrix of set A\n",
    "    :param X_B: array-like, shape=(n_samples_B, d), feature-matrix of set B or None, than Y = X\n",
    "\n",
    "    :return: array-like, shape=(n_samples_A, n_samples_B), kernel matrix\n",
    "    \"\"\"\n",
    "    if X_B is None:\n",
    "        X_B = X_A\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    K_AB = X_A @ X_B.T\n",
    "\n",
    "    return K_AB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd2f5a67d0e15d531a34ded834d51644",
     "grade": true,
     "grade_id": "linear_kernel_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_A = np.array([[1], [2], [3], [4]])\n",
    "X_B = np.array([[0], [2], [1]])\n",
    "\n",
    "# Test size\n",
    "np.testing.assert_equal(linear_kernel(X_A, X_B).shape, (4, 3))\n",
    "np.testing.assert_equal(linear_kernel(X_A).shape, (4, 4))\n",
    "\n",
    "# Test values\n",
    "np.testing.assert_equal(linear_kernel(X_A)[0, 0], 1)\n",
    "np.testing.assert_equal(linear_kernel(X_A)[1, 1], 4)\n",
    "np.testing.assert_equal(linear_kernel(X_A)[0, 2], 3)\n",
    "np.testing.assert_equal(linear_kernel(X_A)[2, 0], 3)\n",
    "\n",
    "np.testing.assert_equal(linear_kernel(X_A, X_B)[0, 0], 0)\n",
    "np.testing.assert_equal(linear_kernel(X_A, X_B)[0, 1], 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Gaussian kernel (1 Point) <a id='task_1_b'></a>\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b>\n",
    "    Implement missing code parts of the function calculation the Gaussian kernel matrix given two feature vector matrices $\\mathbf{X}_A$ and $\\mathbf{X}_B$. The resulting kernel matrix $\\mathbf{K}_{gau}$ must have dimension $n_A\\times n_B$. For a single entry in the kernel matrix it must hold:\n",
    "    <br/><br/>\n",
    "    $$[\\mathbf{K}_{gau}]_{ij}=\\kappa_{gau}(\\mathbf{x}_i,\\mathbf{x}_j)=\\exp\\left(-\\frac{\\|\\mathbf{x}_i-\\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$$\n",
    "    <br/>\n",
    "    with $\\mathbf{x}_i,\\mathbf{x}_j\\in\\mathbb{R}^{d}$ being two examples from set $A$ respecively $B$, and $\\gamma>0$ being the bandwidth parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a640d6b565b8b8d72bb9de309c9cd26",
     "grade": false,
     "grade_id": "gaussian_kernel",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel(X_A, X_B=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian kernel matrix, so that\n",
    "\n",
    "        k_ij = exp(-||x_i - x_j||^2 / (2 * sigma^2))\n",
    "\n",
    "    :param X_A: array-like, shape=(n_samples_A, n_features), feature-matrix of set A\n",
    "    :param X_B: array-like, shape=(n_samples_B, n_features), feature-matrix of set B or None, than X_B = X_A\n",
    "    :param sigma: scalar, bandwidth parameter\n",
    "\n",
    "    :return: array-like, shape=(n_samples_A, n_samples_B), kernel matrix\n",
    "    \"\"\"\n",
    "    if X_B is None:\n",
    "        X_B = X_A\n",
    "\n",
    "    n_A = X_A.shape[0]\n",
    "    n_B = X_B.shape[0]\n",
    "\n",
    "    if sigma is None:\n",
    "        sigma = np.sqrt(X_A.shape[1] / 2.0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    K_A_lin_diag = np.sum(X_A * X_A, axis=1)[:, np.newaxis]  # diag(K_X), shape=(n_A, 1)\n",
    "    K_B_lin_diag = np.sum(X_B * X_B, axis=1)[np.newaxis, :]  # diag(K_Y), shape=(1, n_B)\n",
    "\n",
    "    K_AB_lin = X_A @ X_B.T\n",
    "\n",
    "    K_AB = (K_A_lin_diag @ np.ones((1, n_B)) + np.ones((n_A, 1)) @ K_B_lin_diag - 2 * K_AB_lin)  # ||x_i - x_j||^2\n",
    "    K_AB /= (-2. * sigma**2)\n",
    "    K_AB = np.exp(K_AB)\n",
    "            \n",
    "            \n",
    "    return K_AB\n",
    "\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=0.5)[0, 0], 1)\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=0.5)[0, 2], np.exp(-8))\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, X_B, sigma=0.5)[0, 0], np.exp(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a60e99800689d364bbc899d78b7d5dc1",
     "grade": true,
     "grade_id": "gaussian_kernel_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_A = np.array([[1], [2], [3], [4]])\n",
    "X_B = np.array([[0], [2], [1]])\n",
    "\n",
    "# Test size\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, X_B).shape, (4, 3))\n",
    "np.testing.assert_equal(gaussian_kernel(X_A).shape, (4, 4))\n",
    "\n",
    "# Test values\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=1)[0, 0], 1)\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=1)[1, 1], 1)\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=1)[0, 2], np.exp(-2))\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, sigma=1)[2, 0], np.exp(-2))\n",
    "\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, X_B, sigma=1)[0, 0], np.exp(-0.5))\n",
    "np.testing.assert_equal(gaussian_kernel(X_A, X_B, sigma=1)[0, 1], np.exp(-0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parzen Window Classifier implementation (2 Points) <a id='task_2'></a>\n",
    "The Parzen Window Classifier prediction model can be written as:\n",
    "\n",
    "\n",
    "$$\n",
    "    h(\\mathbf{x})=\\text{sign}(g(\\mathbf{x}))=\\text{sign}\\left(\\sum_{i=1}^{n}\\alpha_i\\kappa(\\mathbf{x}_i,\\mathbf{x})+b\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "with:\n",
    "- $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ being the **decision function**\n",
    "- $b\\in\\mathbb{R}$ being the **bias term** defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "b=\\frac{1}{2n_{{-}}^2}\\sum_{i,j\\in I^{-}}\\kappa(\\mathbf{x}_i,\\mathbf{x}_j)-\\frac{1}{2n_{{+}}^2}\\sum_{i,j\\in I^{+}}\\kappa(\\mathbf{x}_i,\\mathbf{x}_j),\n",
    "$$\n",
    "\n",
    "\n",
    "- $\\alpha_i$'s $\\in\\mathbb{R}$ being the **dual variables** for all training examples $\\mathbf{x}_i$ defined as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\alpha_i=\\begin{cases}\n",
    "            \\frac{1}{n_{{+}}} & \\text{if } y_i=+1\\\\\n",
    "            -\\frac{1}{n_{{-}}} & \\text{if } y_i=-1\n",
    "         \\end{cases}.\n",
    "$$\n",
    "\n",
    "\n",
    "We denote the number of positive / negative training examples as $n_{+}$ / $n_{-}$, and $n=n_{+}+n_{-}$, and $I^{+}$ / $I^{-}$ are the indices of the positive / negative training examples. \n",
    "\n",
    "Below you find the class-template for the Parzen Window Classifier. It's functionality is split into three parts:\n",
    "\n",
    "### 1. Intialization of Classifier Object using **__init__()**\n",
    "A Parzen Window Classifier instance can be created using its constructor and the kernel to be used can be specified, e.g.:\n",
    "```python\n",
    "est = ParzenWindowClassifier(kernel=\"gaussian\").\n",
    "```\n",
    "\n",
    "### 2. Model Training using **fit()** \n",
    "This function takes as input the features of the training examples $\\mathbf{X}_{train}$ and their corresponding labels $\\mathbf{y}_{train}\\in\\{-1,1\\}^{n_{train}}$ and estimates the $\\alpha_i$'s and $b$. The necessary kernel values between the training examples, i.e. $\\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$ are calculated during the fitting process.\n",
    "```python\n",
    "est.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "### 3. Prediction for new Examples using **predict()** and **decision_function()**\n",
    "When the model parameters, i.e. $b$ and $\\alpha_i$s, are fitted, than we can make predictions for a new example $\\mathbf{x}$ using the function $h(\\mathbf{x})$.\n",
    "```python\n",
    "y_test_pred = est.predict(X_test)\n",
    "```\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of <code>fit()</code>, <code>decision_function()</code> and <code>predict()</code>. Make use of the provided formulas (see above).\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hint:</b> The NumPy function <code>np.sum</code> can be used to sum over the elements of a matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0904d070d266be5afc24aaa908bc55ca",
     "grade": false,
     "grade_id": "parzen_classifier",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ParzenWindowClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, kernel=\"gaussian\", sigma=None):\n",
    "        \"\"\"\n",
    "        Parzen Window Classifier\n",
    "\n",
    "        :param kernel: string, specifying which kernel to use. Can be 'gaussian' or 'linear'.\n",
    "        :param sigma: scalar, gaussian kernel parameter, can be None if the linear kernel is used.\n",
    "        \"\"\"\n",
    "        # Parzen Window Classifier model parameter\n",
    "        self.b = None  # bias term\n",
    "        self.alphas = None  # dual variables\n",
    "        \n",
    "        # Training data needed for the prediction phase\n",
    "        self.X_train = None\n",
    "\n",
    "        # Set up kernel function\n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit a Parzen Window Classifier using training data\n",
    "\n",
    "        :param X_train: array-like, shape=(n_samples, n_features), feature-matrix\n",
    "        :param y_train: array-like, shape=(n_samples,) or (n_samples, 1), label vector\n",
    "        \"\"\"\n",
    "        # Calculate the specified kernel\n",
    "        self.X_train = X_train\n",
    "        KX_train = self._get_kernel(self.X_train)\n",
    "\n",
    "        # Get indices of positive and negative examples: I_n, I_p\n",
    "        I_n = (y_train == -1)\n",
    "        I_p = (y_train == 1)\n",
    "\n",
    "        # Count the number of postitive and negative examples: n_n, n_p\n",
    "        n_n = np.sum(I_n)\n",
    "        n_p = np.sum(I_p)\n",
    "\n",
    "        # Calcualte the bias term: self.b\n",
    "        b_n = np.sum(KX_train[I_n][:, I_n]) / (2. * n_n ** 2)\n",
    "        b_p = np.sum(KX_train[I_p][:, I_p]) / (2. * n_p ** 2)\n",
    "        self.b = b_n - b_p\n",
    "\n",
    "        # Calculate alpha_i's: self.alpha\n",
    "        self.alphas = np.zeros((n_n + n_p, 1))\n",
    "        self.alphas[I_n] = -1. / n_n\n",
    "        self.alphas[I_p] = 1. / n_p\n",
    "        \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Calculate decision function:\n",
    "            g(x) = sum_i a_i k(x_i, x) + b\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_test, n_features), feature-matrix of new data.\n",
    "        :return: array-like, shape=(n_samples_test,), decision function value g(x) for all new data points\n",
    "        \"\"\"\n",
    "        if self.alphas is None or self.b is None or self.X_train is None:\n",
    "            raise RuntimeError(\"Call fit-function first.\")\n",
    "\n",
    "        # Calculate the specified kernel between the training and test examples\n",
    "        KX_test_train = self._get_kernel(X, self.X_train)\n",
    "\n",
    "        # Calculate the value of the decision function for each test example\n",
    "        g_X = KX_test_train @ self.alphas + self.b\n",
    "        \n",
    "        return g_X.flatten()  # output a one-dimensional vector       \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels using Parzen Window Classifier:\n",
    "            h(x) = sign(g(x)), with g(x) being the decision function\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_test, n_features), feature-matrix of new data.\n",
    "        :return: array-like, shape=(n_samples_test,), predicted labels {-1, 1} for all new data points\n",
    "        \"\"\"\n",
    "        if self.alphas is None or self.b is None or self.X_train is None:\n",
    "            raise RuntimeError(\"Call fit-function first.\")\n",
    "        \n",
    "        # Calculate prediction h(x) = sign(g(x))\n",
    "        h_X = np.sign(self.decision_function(X))\n",
    "        \n",
    "        return h_X\n",
    "\n",
    "    def _get_kernel(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Calcualte kernel matrix using specified kernel-function and parameters.\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_A, n_features), feature-matrix of set A\n",
    "        :param Y: array-like, shape=(n_samples_B, n_features), feature-matrix of set B or None, than Y = X\n",
    "        :return: array-like, shape=(n_samples_A, n_samples_B), kernel matrix\n",
    "        \"\"\"\n",
    "        if self.kernel == \"gaussian\":\n",
    "            return gaussian_kernel(X, Y, self.sigma)\n",
    "        elif self.kernel == \"linear\":\n",
    "            return linear_kernel(X, Y)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel chosen.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16f447c953adede4c2146db6d2446a40",
     "grade": true,
     "grade_id": "parzen_classifier_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=50, centers=[[1, 1], [1.5, 1.5]], cluster_std=[0.1, 0.1], \n",
    "                  random_state=80)\n",
    "y[y==0] = -1\n",
    "\n",
    "test_est = ParzenWindowClassifier(kernel=\"linear\")\n",
    "test_est.fit(X, y)\n",
    "\n",
    "# Test correct shape of self.b and self.alphas\n",
    "assert(np.isscalar(test_est.b))\n",
    "np.testing.assert_equal(test_est.alphas.shape, (50, 1))\n",
    "\n",
    "# Test correct shape of the predictions\n",
    "X_test = np.array([[1, 1], [1.5, 1.5], [1, 0], [2, 3]])\n",
    "\n",
    "# Validate predictions\n",
    "np.testing.assert_equal(test_est.decision_function(X_test).shape, (4,))\n",
    "\n",
    "np.testing.assert_equal(test_est.predict(X_test).shape, (4,))\n",
    "np.testing.assert_equal(test_est.predict(X_test), np.array([-1, 1, -1, 1]))\n",
    "\n",
    "# Validate score\n",
    "np.testing.assert_equal(test_est.score(X, y), 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application of the Parzen Window Classifier <a id='task_3'></a>\n",
    "It is time to apply your Parzen Window Classifier to some datasets. For that you are given two synthetic datasets $\\mathbf{X}$. Each is splitted into a training and test subset: $\\mathbf{X}_{train}$ (75%) and $\\mathbf{X}_{test}$ (25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data (Please do not change the random_state!)\n",
    "X_blobs, y_blobs = make_blobs(n_samples=500, centers=[[1, 1], [3, 3]], cluster_std=[0.5, 1.15], \n",
    "                              random_state=80)\n",
    "X_moons, y_moons = make_moons(n_samples=500, noise=0.25, random_state=797)\n",
    "\n",
    "# Make labels being {-1, 1}\n",
    "y_blobs[y_blobs==0] = -1\n",
    "y_moons[y_moons==0] = -1\n",
    "\n",
    "# Split data\n",
    "X_blobs_train, X_blobs_test, y_blobs_train, y_blobs_test = train_test_split(\n",
    "    X_blobs, y_blobs, random_state=319)\n",
    "X_moons_train, X_moons_test, y_moons_train, y_moons_test = train_test_split(\n",
    "    X_moons, y_moons, random_state=747)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the datasets to get an impression what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot datasets\n",
    "fig, axrr = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# Blobs\n",
    "for l_str, l_num, col in [(\"negative\", -1, \"blue\"), (\"postive\", 1, \"red\")]:    \n",
    "    axrr[0].scatter(\n",
    "        X_blobs_train[y_blobs_train==l_num, 0], X_blobs_train[y_blobs_train==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Train (%s)\" % l_str, marker=\"s\")\n",
    "    \n",
    "    axrr[0].scatter(\n",
    "        X_blobs_test[y_blobs_test==l_num, 0], X_blobs_test[y_blobs_test==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Test (%s)\" % l_str, marker=\"x\")\n",
    "        \n",
    "# Blobs\n",
    "for l_str, l_num, col in [(\"negative\", -1, \"blue\"), (\"postive\", 1, \"red\")]:    \n",
    "    axrr[1].scatter(\n",
    "        X_moons_train[y_moons_train==l_num, 0], X_moons_train[y_moons_train==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Train (%s)\" % l_str, marker=\"s\")\n",
    "    \n",
    "    axrr[1].scatter(\n",
    "        X_moons_test[y_moons_test==l_num, 0], X_moons_test[y_moons_test==l_num, 1],\n",
    "        c=col, alpha=0.65, label=\"Test (%s)\" % l_str, marker=\"x\")\n",
    "\n",
    "\n",
    "axrr[0].set_title(\"Random blobs\", fontsize=\"xx-large\")\n",
    "axrr[0].legend(title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "\n",
    "axrr[1].set_title(\"Moons\", fontsize=\"xx-large\")\n",
    "axrr[1].legend(title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Implement the hyper parameter optimization (1 point) <a id='task_3_a'></a>\n",
    "Train (fit) your Parzen Window Classifier with Gaussian kernel on the training examples, i.e. `X_blobs_train` and `X_moons_train`. To find the optimal Gaussian bandwidth parameter $\\sigma$ we search a grid of different parameter values and score each one using CV. \n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of the <code>hyper_parameter_search_using_cv</code> function. \n",
    "</div>\n",
    "\n",
    "The function gets in a set of training examples and a grid of different parameters, e.g. $\\sigma$'s, calculates an average validation set score for all of them using cross-validation. Subsequently, a model using all the training data ist used, to train a model with the best set of parameters. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hints:</b>\n",
    "    \n",
    "- Make use of the Python tutorial (fetch on JupyterHub), if you want to see the pseudo-code of the parameter search. \n",
    "- Read the documentation of the sklearn [<code>KFold</code> function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold).\n",
    "- You can calculate the performance score using <code>est.score(...)</code>. Read also: https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html#sklearn.base.ClassifierMixin.score\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4268af327d8a22fc9a9b272f6fb487b2",
     "grade": false,
     "grade_id": "grid_search",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hyper_parameter_search_using_cv(estimator, X, y, param_grid, n_cv_folds=5, \n",
    "                                    random_state=None):\n",
    "    \"\"\"\n",
    "    Function calculating the estimator score for a grid of hyper parameters.\n",
    "\n",
    "    :param estimator: object, subclass of RegressorMixin or ClassifierMixin and BaseEstimator\n",
    "    :param X: array-like, shape=(n_samples, n_features), feature-matrix used for training\n",
    "    :param y: array-like, shape=(n_samples,) or (n_samples, 1), label vector used for training\n",
    "    :param param_grid: dictionary,\n",
    "        keys: different parameter names\n",
    "        values: grid-values for each parameter\n",
    "    :param n_cv_folds: scalar, a KFold cross-validation is performed where the number of splits is equal the scalar.\n",
    "    :param random_state: scalar, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    :return: tuple = (\n",
    "            best estimator,\n",
    "            param grid as list and corresponding evaluation scores,\n",
    "            score of best parameter\n",
    "            best parameter\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Get an iterator over all parameters\n",
    "    param_grid_iter = ParameterGrid(param_grid)\n",
    "\n",
    "    # Create cross-validation object\n",
    "    cv = KFold(n_splits=n_cv_folds, random_state=random_state)\n",
    "    \n",
    "    # Store the valdidation set performance scores for all folds and parameters\n",
    "    perf_scores = np.zeros((cv.get_n_splits(), len(param_grid_iter)))  \n",
    "\n",
    "    for fold, (train_set, val_set) in enumerate(cv.split(X, y)):\n",
    "        # Separate training and validation set from X and y,\n",
    "        #  i.e. X_train, X_val, y_train and y_val\n",
    "        ### BEGIN SOLUTION\n",
    "        X_train = X[train_set]\n",
    "        X_val = X[val_set]\n",
    "\n",
    "        y_train = y[train_set]\n",
    "        y_val = y[val_set]\n",
    "        ### END SOLUTION\n",
    "\n",
    "        for idx, param in enumerate(param_grid_iter):\n",
    "            # Clone the estimator object to get an un-initialized object\n",
    "            est = clone(estimator)\n",
    "\n",
    "            # Set model parameters\n",
    "            est.set_params(**param)\n",
    "\n",
    "            # Fit the model using training set\n",
    "            est.fit(X_train, y_train)\n",
    "\n",
    "            # Calculate the perf. score on validation set for current fold and parameter index\n",
    "            perf_scores[fold, idx] = est.score(X_val, y_val)\n",
    "            \n",
    "    # Find best performing hyper-parameter\n",
    "    \n",
    "    # Average the perf. scores for each parameter across each fold\n",
    "    avg_perf_scores = np.mean(perf_scores, axis=0)\n",
    "    \n",
    "    idx_best = np.argmax(avg_perf_scores)\n",
    "    best_perf_score = avg_perf_scores[idx_best]\n",
    "    best_param = param_grid_iter[idx_best]\n",
    "\n",
    "    # Fit model using all data with the best parameters\n",
    "    est = clone(estimator)\n",
    "    est.set_params(**best_param)\n",
    "    est.fit(X, y)\n",
    "\n",
    "    return (est, {\"params\": list(param_grid_iter), \"scores\": avg_perf_scores}, \n",
    "            best_perf_score, best_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "336c455f7a355096abb357e7d2f8ce09",
     "grade": true,
     "grade_id": "grid_search_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=[[1, 1], [1.5, 1.5]], cluster_std=[0.2, 0.75], \n",
    "                  random_state=100)\n",
    "y[y==0] = -1\n",
    "\n",
    "_param_grid = {\"sigma\": [0.05, 0.1, 0.5, 1.]}\n",
    "_est, _param_scores, _best_score, _best_param = hyper_parameter_search_using_cv(\n",
    "    ParzenWindowClassifier(kernel=\"gaussian\"), X, y, _param_grid)\n",
    "\n",
    "np.testing.assert_equal(_param_scores[\"scores\"].shape, (len(_param_grid[\"sigma\"]),))\n",
    "np.testing.assert_allclose(_param_scores[\"scores\"], np.array([0.83, 0.91, 0.89, 0.83]))\n",
    "np.testing.assert_equal(_best_param, {\"sigma\": 0.1})\n",
    "np.testing.assert_allclose(_best_score, 0.91)\n",
    "assert(isinstance(_est, ParzenWindowClassifier))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Plot validation score for different hyper parameters <a id='task_3_b'></a>\n",
    "With the hyper parameter optimization function at hand, we find the best $\\sigma$ parameter for the two synthetic datasets and also inspect the average validation error for different values of $\\sigma$.\n",
    "\n",
    "First let us the parameter grid for the Gaussian kernel bandwidth parameter $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"sigma\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the `hyper_parameter_search_using_cv` function for `X_blobs_train` and `X_blobs_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_blobs, scores_blobs, best_score_blobs, best_param_blobs = hyper_parameter_search_using_cv(\n",
    "    ParzenWindowClassifier(kernel=\"gaussian\"), X_blobs_train, y_blobs_train, param_grid,\n",
    "    random_state=737)\n",
    "\n",
    "print(\"[Blobs] Best average validation score\", best_score_blobs)\n",
    "print(\"[Blobs] Best parameter\", best_param_blobs)\n",
    "\n",
    "est_moons, scores_moons, best_score_moons, best_param_moons = hyper_parameter_search_using_cv(\n",
    "    ParzenWindowClassifier(kernel=\"gaussian\"), X_moons_train, y_moons_train, param_grid,\n",
    "    random_state=747)\n",
    "\n",
    "print(\"[Moons] Best average validation score\", best_score_moons)\n",
    "print(\"[Moons] Best parameter\", best_param_moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot validation score for the different parameter values. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hint:</b> Both curves should have a single maxima, if you did not change any random seeds, i.e. <code>random_state</code> values, or data generation parameters. Otherwise, it could be that multiple $\\sigma$'s are equally good. However, overall it should be a function with single maxima (maybe a region).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(1, 2, figsize=(14, 4), sharex=\"row\")\n",
    "\n",
    "axrr[0].plot(param_grid[\"sigma\"], scores_blobs[\"scores\"], '*-')\n",
    "axrr[0].set_xscale(\"log\")\n",
    "axrr[0].grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "axrr[0].set_title(\"Random blobs: Validation accuracy\")\n",
    "axrr[0].set_xlabel(\"Gaussian bandwidth parameter\")\n",
    "axrr[0].set_ylabel(\"Mean accuracy (perf. score)\")\n",
    "\n",
    "axrr[1].plot(param_grid[\"sigma\"], scores_moons[\"scores\"], '*-')\n",
    "axrr[1].set_xscale(\"log\")\n",
    "axrr[1].grid(True, which=\"both\", ls=\"-\", alpha=0.5)\n",
    "axrr[1].set_title(\"Moons: Validation accuracy\")\n",
    "axrr[1].set_xlabel(\"Gaussian bandwidth parameter\")\n",
    "axrr[1].set_ylabel(\"Mean accuracy (perf. score)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyper-parameter optimization was done only using the training data. We now can apply the best model (with the optimal $\\sigma$ parameter) to the training data. We should see a similar performance as for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Blobs] Score on test set\", est_blobs.score(X_blobs_test, y_blobs_test))\n",
    "print(\"[Moons] Score on test set\", est_moons.score(X_moons_test, y_moons_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model visualization: Non-linear vs. Linear (**no points**)  <a id='task_4'></a>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Bonus task:</b>\n",
    "    \n",
    "Here we inspect the decision function and how it separates the two classes in the feature space. For that we will evaluate $g(\\mathbf{x})$ for $\\mathbf{x}$'s on a regular grid. \n",
    "\n",
    "1. Set up interval to plot decision function, e.g. min. and max. value of our synthetic datasets. \n",
    "2. Create grid-points $\\mathbf{x}$ covering the intervall. \n",
    "3. Evaluate $z=g(\\mathbf{x})$ for all points.\n",
    "4. Plot the $z$'s for different points and color them according to their sign.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gridpoints(X):\n",
    "    \"\"\"\n",
    "    :param X: array-like, shape=(n_samples, d), datset feature matrix\n",
    "    \n",
    "    :return: array-like, shape=(n_grid_samples, d), feature vectors on a regular grid in the feature space\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    \n",
    "    return xx, yy\n",
    "\n",
    "def get_color_normalizer(Z, n_colors=256, n_point=10):\n",
    "    bounds = np.append(np.linspace(np.min(Z), 0, n_point)[:-1], np.linspace(0, np.max(Z), n_point))  # do not add zero twice\n",
    "    return colors.BoundaryNorm(boundaries=bounds, ncolors=n_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function for Gaussian Kernel (Model of previous tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_blobs_grid, YY_blobs_grid = get_gridpoints(X_blobs)\n",
    "Z_blobs_grid = est_blobs.decision_function(np.c_[XX_blobs_grid.ravel(), YY_blobs_grid.ravel()])\n",
    "Z_blobs_grid = Z_blobs_grid.reshape(XX_blobs_grid.shape)\n",
    "\n",
    "XX_moons_grid, YY_moons_grid = get_gridpoints(X_moons)\n",
    "Z_moons_grid = est_moons.decision_function(np.c_[XX_moons_grid.ravel(), YY_moons_grid.ravel()])\n",
    "Z_moons_grid = Z_moons_grid.reshape(XX_moons_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# plot contours, levels, ...\n",
    "pcm = axrr[0].pcolormesh(XX_blobs_grid, YY_blobs_grid, Z_blobs_grid, alpha=0.5, shading=\"gouraud\",\n",
    "                   norm=get_color_normalizer(Z_blobs_grid))\n",
    "fig.colorbar(pcm, ax=axrr[0], extend=\"both\", orientation=\"vertical\")\n",
    "axrr[0].contour(XX_blobs_grid, YY_blobs_grid, Z_blobs_grid, colors=['k'], linestyles=['-'], levels=[0])\n",
    "\n",
    "# plot points\n",
    "axrr[0].scatter(X_blobs_train[:, 0], X_blobs_train[:, 1], c=y_blobs_train, marker=\"s\", edgecolors=\"k\", alpha=0.35)\n",
    "axrr[0].scatter(X_blobs_test[:, 0], X_blobs_test[:, 1], c=y_blobs_test, marker=\"x\", edgecolors=\"k\")\n",
    "\n",
    "# plot labels, titles, ...\n",
    "axrr[0].legend([\"Training\", \"Test\"], title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "axrr[0].set_title(\"Random Blobs: Decision function (sigma=%.3f)\" % best_param_blobs[\"sigma\"],\n",
    "                 fontsize=\"xx-large\")\n",
    "\n",
    "pcm = axrr[1].pcolormesh(XX_moons_grid, YY_moons_grid, Z_moons_grid, alpha=0.5, shading=\"gouraud\",\n",
    "                   norm=get_color_normalizer(Z_moons_grid))\n",
    "fig.colorbar(pcm, ax=axrr[1], extend=\"both\", orientation=\"vertical\")\n",
    "axrr[1].scatter(X_moons_train[:, 0], X_moons_train[:, 1], c=y_moons_train, marker=\"s\", edgecolors=\"k\", alpha=0.35)\n",
    "axrr[1].scatter(X_moons_test[:, 0], X_moons_test[:, 1], c=y_moons_test, edgecolors=\"k\", marker=\"x\")\n",
    "axrr[1].legend([\"Training\", \"Test\"], title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "axrr[1].set_title(\"Moons: Decision function (sigma=%.3f)\" % best_param_moons[\"sigma\"],\n",
    "                 fontsize=\"xx-large\")\n",
    "axrr[1].contour(XX_moons_grid, YY_moons_grid, Z_moons_grid, colors=['k'], linestyles=['-'], levels=[0])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function for Linear Kernel\n",
    "We have seen that the Gaussian kernel leads to a non-linear decision boundary, i.e. $g(\\mathbf{x})=0$ (black line in plots). Now we take a look on the linear Parzen Window Classifier, buy using a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_blobs = ParzenWindowClassifier(kernel=\"linear\")\n",
    "est_blobs.fit(X_blobs_train, y_blobs_train)\n",
    "\n",
    "est_moons = ParzenWindowClassifier(kernel=\"linear\")\n",
    "est_moons.fit(X_moons_train, y_moons_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_blobs_grid, YY_blobs_grid = get_gridpoints(X_blobs)\n",
    "Z_blobs_grid = est_blobs.decision_function(np.c_[XX_blobs_grid.ravel(), YY_blobs_grid.ravel()])\n",
    "Z_blobs_grid = Z_blobs_grid.reshape(XX_blobs_grid.shape)\n",
    "\n",
    "XX_moons_grid, YY_moons_grid = get_gridpoints(X_moons)\n",
    "Z_moons_grid = est_moons.decision_function(np.c_[XX_moons_grid.ravel(), YY_moons_grid.ravel()])\n",
    "Z_moons_grid = Z_moons_grid.reshape(XX_moons_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(1, 2, figsize=(20, 7))\n",
    "\n",
    "# plot contours, levels, ...\n",
    "pcm = axrr[0].pcolormesh(XX_blobs_grid, YY_blobs_grid, Z_blobs_grid, alpha=0.5, shading=\"gouraud\",\n",
    "                   norm=get_color_normalizer(Z_blobs_grid))\n",
    "fig.colorbar(pcm, ax=axrr[0], extend=\"both\", orientation=\"vertical\")\n",
    "axrr[0].contour(XX_blobs_grid, YY_blobs_grid, Z_blobs_grid, colors=['k'], linestyles=['-'], levels=[0])\n",
    "\n",
    "# plot points\n",
    "axrr[0].scatter(X_blobs_train[:, 0], X_blobs_train[:, 1], c=y_blobs_train, marker=\"s\", edgecolors=\"k\", alpha=0.35)\n",
    "axrr[0].scatter(X_blobs_test[:, 0], X_blobs_test[:, 1], c=y_blobs_test, edgecolors=\"k\", marker=\"x\")\n",
    "\n",
    "# plot labels, titles, ...\n",
    "axrr[0].legend([\"Training\", \"Test\"], title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "axrr[0].set_title(\"Random Blobs: Decision function (linear kernel)\", fontsize=\"xx-large\")\n",
    "\n",
    "pcm = axrr[1].pcolormesh(XX_moons_grid, YY_moons_grid, Z_moons_grid, alpha=0.5, shading=\"gouraud\",\n",
    "                   norm=get_color_normalizer(Z_moons_grid))\n",
    "fig.colorbar(pcm, ax=axrr[1], extend=\"both\", orientation=\"vertical\")\n",
    "axrr[1].scatter(X_moons_train[:, 0], X_moons_train[:, 1], c=y_moons_train, marker=\"s\", edgecolors=\"k\", alpha=0.35)\n",
    "axrr[1].scatter(X_moons_test[:, 0], X_moons_test[:, 1], c=y_moons_test, edgecolors=\"k\", marker=\"x\")\n",
    "axrr[1].legend([\"Training\", \"Test\"], title=\"Data points\", fontsize=\"x-large\", scatterpoints=3, edgecolor=\"k\")\n",
    "axrr[1].set_title(\"Moons: Decision function (linear kernel)\", fontsize=\"xx-large\")\n",
    "axrr[1].contour(XX_moons_grid, YY_moons_grid, Z_moons_grid, colors=['k'], linestyles=['-'], levels=[0])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
